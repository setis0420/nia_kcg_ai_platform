import os, gc
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# =========================
# í•˜ì´í¼íŒŒë¼ë¯¸í„°
# =========================
SEQ_LEN    = 80
STEP_SIZE  = 3
EPOCHS     = 10
BATCH_SIZE = 256
LR         = 1e-3

SAVE_DIR = "global_model"
os.makedirs(SAVE_DIR, exist_ok=True)

# âœ… ì¢Œìš° ë°˜ì „ ë³´ì •(ë‚¨ë¶ ìœ ì§€, ë™ì„œ ë°˜ì „)
# - ì¦ìƒ: ì¢Œìš°ë§Œ ë°˜ëŒ€ë¡œ ê°€ëŠ” ê²½ìš° Trueë¡œ ë‘ëŠ” ê²Œ ë§ìŒ
COG_MIRROR = True

# (ì„ íƒ) í­ì£¼ ë°©ì§€
GRAD_CLIP_NORM = 1.0


# =========================
# 1-A. segment ë¶„ë¦¬
# =========================
def split_by_gap(df, max_gap_days=1):
    if df is None or df.empty:
        return []
    df = df.sort_values("datetime").copy()
    df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")
    df = df.dropna(subset=["datetime"])
    if df.empty:
        return []

    diff = df["datetime"].diff()
    new_seg = (diff >= pd.Timedelta(days=max_gap_days)) | diff.isna()
    seg_id = new_seg.cumsum()

    return [g.copy() for _, g in df.groupby(seg_id) if len(g) > 0]


# =========================
# 1-B. 1ë¶„ ë³´ê°„ (FULL series)
# =========================
def data_intp(df):
    if df is None or df.empty:
        return None

    df = df.drop_duplicates(subset=["datetime", "lat", "lon", "sog", "cog"], keep="first")
    df = df.sort_values("datetime").copy()
    df["datetime"] = pd.to_datetime(df["datetime"], errors="coerce")

    for col in ["lat", "lon", "sog", "cog"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    keep_cols = [c for c in df.columns if c in ["datetime","mmsi","lat","lon","sog","cog","fid"]]
    df = df[keep_cols].dropna(subset=["datetime", "lat", "lon", "sog", "cog"])
    if df.empty:
        return None

    dt_range = pd.date_range(
        start=df["datetime"].iloc[0].floor("T"),
        end=df["datetime"].iloc[-1].ceil("T"),
        freq="1min"
    )

    range_df = pd.DataFrame({"datetime": dt_range})
    # âœ… segment ë‹¨ìœ„ë¡œ mmsi/fid ê³ ì •
    range_df["mmsi"] = df["mmsi"].iloc[0] if "mmsi" in df.columns else np.nan
    range_df["fid"]  = df["fid"].iloc[0]  if "fid"  in df.columns else np.nan

    merge_df = (
        pd.concat([df, range_df], axis=0)
          .set_index("datetime")
          .sort_index()
    )

    for col in ["lat", "lon", "sog", "cog"]:
        merge_df[col] = pd.to_numeric(merge_df[col], errors="coerce")

    # cog ë³´ê°„ ì•ˆì •í™”: sin/cos ë³´ê°„ í›„ ê°ë„ë¡œ ë³µì›
    merge_df["sin_course"] = np.sin(np.radians(merge_df["cog"]))
    merge_df["cos_course"] = np.cos(np.radians(merge_df["cog"]))

    exclude_cols = ["mmsi", "fid"]
    convert_cols = [c for c in merge_df.columns if c not in exclude_cols]
    merge_df[convert_cols] = merge_df[convert_cols].astype("float")

    intp_df = merge_df.interpolate(method="linear")
    intp_df["cog"] = np.degrees(np.arctan2(intp_df["sin_course"], intp_df["cos_course"]))
    intp_df["cog"] = (intp_df["cog"] + 360) % 360

    intp_df = intp_df.drop(columns=["sin_course","cos_course"], errors="ignore").reset_index()
    intp_df = intp_df.dropna(subset=["lat","lon","sog","cog"])
    return intp_df

# =========================
# 2. Dataset / Model (segment-aware + smoothness ì§€ì›)
# =========================
class TrajectoryDataset(Dataset):
    """
    - segment_bounds: [(s,e), ...]  (intp_all ê¸°ì¤€)
    - start_indicesë¥¼ segment ë³„ë¡œ ë§Œë“¤ì–´ë‘ê³ ,
    - train/val splitì€ segment ë‹¨ìœ„ë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì§€ì›
    """
    def __init__(self, df, seq_len=80, step=3, segment_bounds=None, cog_mirror=False):
        self.seq_len = seq_len
        self.step = step
        self.cog_mirror = cog_mirror

        df = df.copy()

        sin_cog = np.sin(np.radians(df["cog"].values))
        cos_cog = np.cos(np.radians(df["cog"].values))
        if cog_mirror:
            sin_cog = -sin_cog  # ì¢Œìš°ë°˜ì „ ë³´ì •

        df["sin_cog"] = sin_cog
        df["cos_cog"] = cos_cog

        self.feature_cols = ["lat","lon","sog","sin_cog","cos_cog"]
        self.target_cols  = ["lat","lon","sog","sin_cog","cos_cog"]

        X = df[self.feature_cols].values.astype(np.float32)
        Y = df[self.target_cols].values.astype(np.float32)

        self.x_mean = X.mean(axis=0, keepdims=True)
        self.x_std  = X.std(axis=0, keepdims=True) + 1e-6
        self.y_mean = Y.mean(axis=0, keepdims=True)
        self.y_std  = Y.std(axis=0, keepdims=True) + 1e-6

        self.Xn = (X - self.x_mean) / self.x_std
        self.Yn = (Y - self.y_mean) / self.y_std

        if segment_bounds is None:
            segment_bounds = [(0, len(self.Xn))]
        self.segment_bounds = segment_bounds

        # segmentë³„ start index ëª©ë¡ì„ ë”°ë¡œ ì €ì¥ (segment splitìš©)
        self.segment_starts = []  # list[list[int]]
        for (s, e) in self.segment_bounds:
            starts = []
            max_start = e - 1 - self.seq_len
            if max_start >= s:
                for i in range(s, max_start + 1, self.step):
                    starts.append(i)
            self.segment_starts.append(starts)

        # ê¸°ë³¸ì€ ì „ì²´ flatten
        self.start_indices = [i for starts in self.segment_starts for i in starts]

    def set_active_segments(self, active_segment_ids):
        """train/valì„ segment ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ë’¤, í•´ë‹¹ segmentë“¤ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •"""
        self.start_indices = []
        for sid in active_segment_ids:
            self.start_indices.extend(self.segment_starts[sid])

    def __len__(self):
        return len(self.start_indices)

    def __getitem__(self, idx):
        s = self.start_indices[idx]
        e = s + self.seq_len
        x = self.Xn[s:e]     # (seq_len,5)
        y = self.Yn[e]       # (5,)
        x_last = self.Xn[e-1]  # (5,)  smoothnessì— í•„ìš”: ì§ì „ ìƒíƒœ
        return torch.from_numpy(x), torch.from_numpy(y), torch.from_numpy(x_last)


class LSTMTrajectoryModel(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=128, num_layers=2, output_dim=5, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


# =========================
# Loss: weighted mse + smoothness penalty + ë³€ì¹¨ í•™ìŠµ ê°•í™”
# =========================
def loss_with_smoothness(y_pred, y_true, x_last,
                         w_mse=(2, 2, 1, 3, 3),  # sin/cos ê°€ì¤‘ì¹˜ ì¦ê°€ (1->3)
                         smooth_lambda=0.05,
                         sog_lambda=0.10,
                         heading_lambda=0.02,  # ì¹¨ë¡œ smoothness ê°ì†Œ (0.05->0.02)
                         turn_boost=2.0):  # ë³€ì¹¨ êµ¬ê°„ ê°€ì¤‘ì¹˜
    """
    y_pred, y_true: (B,5)  [lat, lon, sog, sin, cos]
    x_last: (B,5)  ì§ì „ ìƒíƒœ(ì •ê·œí™”ëœ ê°’)

    ë³€ì¹¨ í•™ìŠµ ê°•í™”:
    - sin/cos ê°€ì¤‘ì¹˜ ì¦ê°€
    - heading smoothness ê°ì†Œ (ë³€ì¹¨ í—ˆìš©)
    - ë³€ì¹¨ êµ¬ê°„(ì¹¨ë¡œ ë³€í™”ê°€ í° ìƒ˜í”Œ)ì— ì¶”ê°€ ê°€ì¤‘ì¹˜
    """

    # (1) weighted MSE
    w = torch.tensor(w_mse, device=y_pred.device, dtype=y_pred.dtype).view(1, -1)

    # ë³€ì¹¨ êµ¬ê°„ ê°ì§€: ì‹¤ì œ ì¹¨ë¡œ ë³€í™”ëŸ‰ ê³„ì‚°
    # x_lastì™€ y_trueì˜ sin/cos ì°¨ì´ê°€ í¬ë©´ ë³€ì¹¨ êµ¬ê°„
    true_heading_change = ((y_true[:, 3:5] - x_last[:, 3:5]) ** 2).sum(dim=1).sqrt()

    # ë³€ì¹¨ êµ¬ê°„ì— ì¶”ê°€ ê°€ì¤‘ì¹˜ (ë³€í™”ëŸ‰ì´ í´ìˆ˜ë¡ ë” ì¤‘ìš”í•˜ê²Œ í•™ìŠµ)
    turn_weight = 1.0 + turn_boost * true_heading_change
    turn_weight = turn_weight.unsqueeze(1)  # (B, 1)

    # ê°€ì¤‘ MSE (ë³€ì¹¨ êµ¬ê°„ ê°•ì¡°)
    mse = ((y_pred - y_true) ** 2 * w * turn_weight).mean()

    # (2) smoothness: ì§ì§„ êµ¬ê°„ì—ì„œë§Œ ì ìš© (ë³€ì¹¨ êµ¬ê°„ì—ì„œëŠ” ì•½í•˜ê²Œ)
    dsog = (y_pred[:, 2] - x_last[:, 2]).abs().mean()
    dheading = (y_pred[:, 3:5] - x_last[:, 3:5]).abs().mean()

    smooth = sog_lambda * dsog + heading_lambda * dheading
    return mse + smooth_lambda * smooth


# =========================
# 3. Global í•™ìŠµ í•¨ìˆ˜ (segment split + early stop + scheduler + smoothness)
# =========================
def train_global_model(
    df_all,
    seq_len=SEQ_LEN, step_size=STEP_SIZE,
    epochs=300,
    batch_size=BATCH_SIZE, lr=LR,
    save_dir=SAVE_DIR,
    device=None,
    cog_mirror=COG_MIRROR,

    # early stop
    patience=30,
    min_delta=1e-5,
    warmup_epochs=20,

    # scheduler
    use_scheduler=True,
    lr_patience=8,
    lr_factor=0.5,
    min_lr=1e-6,

    # smoothness & ë³€ì¹¨ í•™ìŠµ
    smooth_lambda=0.05,
    sog_lambda=0.10,
    heading_lambda=0.02,  # ì¹¨ë¡œ smoothness ê°ì†Œ (ë³€ì¹¨ í—ˆìš©)
    turn_boost=2.0,       # ë³€ì¹¨ êµ¬ê°„ ê°€ì¤‘ì¹˜ (í´ìˆ˜ë¡ ë³€ì¹¨ í•™ìŠµ ê°•í™”)

    grad_clip_norm=1.0,
    val_ratio=0.2,
    seed=42,
):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    # --------------------
    # ë°ì´í„° ì¤€ë¹„ + ë³´ê°„ + segment_bounds ìƒì„± (ë„¤ ì½”ë“œ ê·¸ëŒ€ë¡œ)
    # --------------------
    required = ["datetime","mmsi","lat","lon","sog","cog","fid"]
    missing = [c for c in required if c not in df_all.columns]
    if missing:
        raise ValueError(f"df_allì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

    df_all = df_all.copy()
    df_all["datetime"] = pd.to_datetime(df_all["datetime"], errors="coerce")
    for c in ["lat","lon","sog","cog","mmsi","fid"]:
        df_all[c] = pd.to_numeric(df_all[c], errors="coerce")
    df_all = df_all.dropna(subset=["datetime","lat","lon","sog","cog","fid","mmsi"])
    df_all = df_all.sort_values(["fid","datetime"]).reset_index(drop=True)

    print(f"[GLOBAL] ì›ë³¸ rows={len(df_all)}, fid={df_all.fid.nunique()}, mmsi={df_all.mmsi.nunique()}")

    intp_segments, seg_lengths = [], []

    for fid, df_fid in df_all.groupby("fid"):
        segments_raw = split_by_gap(df_fid, max_gap_days=1)
        for seg_df in segments_raw:
            intp_df = data_intp(seg_df)
            if intp_df is None or len(intp_df) == 0:
                continue
            intp_df = intp_df.sort_values("datetime").reset_index(drop=True)
            intp_segments.append(intp_df)
            seg_lengths.append(len(intp_df))

    if len(intp_segments) == 0:
        raise RuntimeError("[GLOBAL] ë³´ê°„ëœ segmentê°€ ì—†ìŠµë‹ˆë‹¤.")

    intp_all = pd.concat(intp_segments, ignore_index=True)

    segment_bounds = []
    s = 0
    for L in seg_lengths:
        e = s + L
        segment_bounds.append((s, e))
        s = e

    print(f"[GLOBAL] ë³´ê°„ í›„ rows={len(intp_all)}, segments={len(segment_bounds)}")

    # --------------------
    # âœ… í•­ë¡œ ë²”ìœ„ ê³„ì‚° (ì¶”ë¡  ì‹œ ìœ¡ì§€ ì¹¨ë²” ë°©ì§€ìš©)
    # --------------------
    lat_min, lat_max = intp_all["lat"].min(), intp_all["lat"].max()
    lon_min, lon_max = intp_all["lon"].min(), intp_all["lon"].max()
    # ì•½ê°„ì˜ ë§ˆì§„ ì¶”ê°€ (0.01ë„ â‰ˆ 1.1km)
    lat_margin = (lat_max - lat_min) * 0.05
    lon_margin = (lon_max - lon_min) * 0.05
    lat_bounds = (lat_min - lat_margin, lat_max + lat_margin)
    lon_bounds = (lon_min - lon_margin, lon_max + lon_margin)
    print(f"[GLOBAL] í•­ë¡œ ë²”ìœ„: lat={lat_bounds[0]:.4f}~{lat_bounds[1]:.4f}, lon={lon_bounds[0]:.4f}~{lon_bounds[1]:.4f}")

    # --------------------
    # âœ… Dataset ìƒì„± (segment-aware)
    # --------------------
    dataset = TrajectoryDataset(
        intp_all,
        seq_len=seq_len,
        step=step_size,
        segment_bounds=segment_bounds,
        cog_mirror=cog_mirror,
    )

    n_segments = len(dataset.segment_starts)
    if n_segments <= 1:
        raise RuntimeError(f"[GLOBAL] segments={n_segments} ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. split ë¶ˆê°€")

    # --------------------
    # âœ… Segment ë‹¨ìœ„ train/val split
    # --------------------
    rng = np.random.default_rng(seed)
    seg_ids = np.arange(n_segments)
    rng.shuffle(seg_ids)

    n_val_seg = max(1, int(n_segments * val_ratio))
    val_seg_ids = seg_ids[:n_val_seg].tolist()
    train_seg_ids = seg_ids[n_val_seg:].tolist()

    # trainìš© dataset / valìš© datasetì„ ë³µì œí•´ì„œ ì‚¬ìš©
    train_ds = dataset
    val_ds = TrajectoryDataset(
        intp_all,
        seq_len=seq_len,
        step=step_size,
        segment_bounds=segment_bounds,
        cog_mirror=cog_mirror,
    )

    train_ds.set_active_segments(train_seg_ids)
    val_ds.set_active_segments(val_seg_ids)

    if len(train_ds) == 0 or len(val_ds) == 0:
        raise RuntimeError(f"[GLOBAL] train/val ì‹œí€€ìŠ¤ê°€ 0ì…ë‹ˆë‹¤. (train={len(train_ds)}, val={len(val_ds)})")

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

    print(f"[GLOBAL] train segments={len(train_seg_ids)}, val segments={len(val_seg_ids)}")
    print(f"[GLOBAL] train seq={len(train_ds)}, val seq={len(val_ds)}")

    # --------------------
    # ëª¨ë¸/ì˜µí‹°ë§ˆ/ìŠ¤ì¼€ì¤„ëŸ¬
    # --------------------
    model = LSTMTrajectoryModel().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    scheduler = None
    if use_scheduler:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode="min", factor=lr_factor, patience=lr_patience, min_lr=min_lr, verbose=True
        )

    # --------------------
    # Early stop
    # --------------------
    best_val = float("inf")
    best_epoch = -1
    bad_count = 0
    best_state = None

    print(f"[GLOBAL] í•™ìŠµ ì‹œì‘ | max_epochs={epochs}, warmup={warmup_epochs}, patience={patience}, device={device}")

    for epoch in range(1, epochs + 1):
        # ---- train
        model.train()
        tr_loss = 0.0
        for x, y, x_last in train_loader:
            x, y, x_last = x.to(device), y.to(device), x_last.to(device)

            optimizer.zero_grad()
            y_pred = model(x)
            loss = loss_with_smoothness(
                y_pred, y, x_last,
                smooth_lambda=smooth_lambda,
                sog_lambda=sog_lambda,
                heading_lambda=heading_lambda,
                turn_boost=turn_boost,
            )
            loss.backward()

            if grad_clip_norm and grad_clip_norm > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)

            optimizer.step()
            tr_loss += loss.item() * x.size(0)

        tr_loss /= max(1, len(train_loader.dataset))

        # ---- val
        model.eval()
        va_loss = 0.0
        with torch.inference_mode():
            for x, y, x_last in val_loader:
                x, y, x_last = x.to(device), y.to(device), x_last.to(device)
                y_pred = model(x)
                loss = loss_with_smoothness(
                    y_pred, y, x_last,
                    smooth_lambda=smooth_lambda,
                    sog_lambda=sog_lambda,
                    heading_lambda=heading_lambda,
                    turn_boost=turn_boost,
                )
                va_loss += loss.item() * x.size(0)
        va_loss /= max(1, len(val_loader.dataset))

        cur_lr = optimizer.param_groups[0]["lr"]

        # Epoch í•™ìŠµ ê²°ê³¼ ì¶œë ¥
        print("=" * 70)
        print(f"  Epoch: {epoch:03d} / {epochs}")
        print(f"  Train Loss: {tr_loss:.6f}")
        print(f"  Val Loss:   {va_loss:.6f}")
        print(f"  Learning Rate: {cur_lr:.2e}")
        if best_val < float("inf"):
            print(f"  Best Val Loss: {best_val:.6f} (Epoch {best_epoch})")
        print("=" * 70)

        if scheduler is not None:
            scheduler.step(va_loss)

        improved = (best_val - va_loss) > min_delta
        if improved:
            best_val = va_loss
            best_epoch = epoch
            bad_count = 0
            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
        else:
            bad_count += 1

        if epoch >= warmup_epochs and bad_count >= patience:
            print(f"[GLOBAL] ğŸ›‘ Early stop at epoch {epoch} (best={best_epoch}, val={best_val:.6f})")
            break

        if cur_lr <= min_lr + 1e-12 and epoch >= warmup_epochs:
            print(f"[GLOBAL] ğŸ›‘ Stop: lr reached min_lr (best={best_epoch}, val={best_val:.6f})")
            break

    # best ë³µì›
    if best_state is not None:
        model.load_state_dict(best_state)

    # ì €ì¥(best ê¸°ì¤€)
    os.makedirs(save_dir, exist_ok=True)
    model_path  = os.path.join(save_dir, "lstm_global.pth")
    scaler_path = os.path.join(save_dir, "scaler_global.npz")

    torch.save(model.state_dict(), model_path)
    np.savez(
        scaler_path,
        x_mean=train_ds.x_mean, x_std=train_ds.x_std,   # âœ… train ê¸°ì¤€ ìŠ¤ì¼€ì¼ëŸ¬(ì¼ê´€)
        y_mean=train_ds.y_mean, y_std=train_ds.y_std,
        seq_len=int(seq_len),
        step=int(step_size),
        feature_cols=np.array(train_ds.feature_cols),
        target_cols=np.array(train_ds.target_cols),
        cog_mirror=bool(cog_mirror),
        best_epoch=int(best_epoch),
        best_val=float(best_val),

        # smoothness meta
        smooth_lambda=float(smooth_lambda),
        sog_lambda=float(sog_lambda),
        heading_lambda=float(heading_lambda),

        # í•­ë¡œ ë²”ìœ„ (ì¶”ë¡  ì‹œ ìœ¡ì§€ ì¹¨ë²” ë°©ì§€ìš©)
        lat_bounds=np.array(lat_bounds),
        lon_bounds=np.array(lon_bounds),
    )

    print(f"[GLOBAL] âœ… ì €ì¥ ì™„ë£Œ(best) epoch={best_epoch}, val={best_val:.6f}")
    print(f"  - {model_path}")
    print(f"  - {scaler_path}")

    del model, optimizer, train_loader, val_loader, train_ds, val_ds, dataset, intp_all
    gc.collect()
    return model_path, scaler_path


